{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Informa\u00e7\u00f5es Georreferenciadas \u00b6 Curso: Engenharia Da Computa\u00e7\u00e3o Professor: Andr\u00e9 Filipe de Moraes Batista - andrefmb (at) insper.edu.br \u00a9 Todos os direitos reservados Objetivos \u00b6 Ao final da disciplina o aluno ser\u00e1 capaz de: Compreender dados espaciais, n\u00e3o-espaciais e a rela\u00e7\u00e3o entre ambos; Converter dados georreferenciados simples em medidas de \u00e1reas deliminatas, localiza\u00e7\u00e3o, dist\u00e2ncia e intera\u00e7\u00e3o espacial; Preprocessar dados georreferenciados usando ferramentas especializadas; Aquisi\u00e7\u00e3o, tratamento e armazenamento de dados de IoT e de dispositivos m\u00f3veis; Realizar an\u00e1lise explorat\u00f3ria de dados georreferenciados e espaciais usando ferramentas especializadas; REalizar an\u00e1lises de clustering de dados usando ambiente de an\u00e1lise (Python) Conte\u00fado Program\u00e1tico \u00b6 Diferen\u00e7a entre dados espaciais e dados n\u00e3o-espaciais No\u00e7\u00f5es b\u00e1sicas de cartografia e sistemas de coordenadas espaciais Fontes de dados georreferenciados: reposit\u00f3rios, dispositivos m\u00f3veis, IoT Trabalhando com formatos comuns de dados espaciais em bases de dados Geocodifica\u00e7\u00e3o direta e reversa Agrega\u00e7\u00e3o de dados: pontos, linhas, pol\u00edgonos An\u00e1lise explorat\u00f3ria de dados espaciais An\u00e1lise de distribui\u00e7\u00f5es espaciais Autocorrela\u00e7\u00e3o espacial Clusters espaciais Bibliografia \u00b6 LAWHEAD, J. Learning Geospatial Analysis with Python: Understand GIS fundamentals and perform remote sensing data analysis using Python. 3.a ed. Packt Publishing, 2019. CRICKARD, P.; VAN REES, E.; TOMS, S. Mastering Geospatial Analysis with Python. 1.a ed. Packt publishing, 2017. CRESSIE, N. Statistics for Spatial Data. 2015.","title":"Home"},{"location":"#informacoes-georreferenciadas","text":"Curso: Engenharia Da Computa\u00e7\u00e3o Professor: Andr\u00e9 Filipe de Moraes Batista - andrefmb (at) insper.edu.br \u00a9 Todos os direitos reservados","title":"Informa\u00e7\u00f5es Georreferenciadas"},{"location":"#objetivos","text":"Ao final da disciplina o aluno ser\u00e1 capaz de: Compreender dados espaciais, n\u00e3o-espaciais e a rela\u00e7\u00e3o entre ambos; Converter dados georreferenciados simples em medidas de \u00e1reas deliminatas, localiza\u00e7\u00e3o, dist\u00e2ncia e intera\u00e7\u00e3o espacial; Preprocessar dados georreferenciados usando ferramentas especializadas; Aquisi\u00e7\u00e3o, tratamento e armazenamento de dados de IoT e de dispositivos m\u00f3veis; Realizar an\u00e1lise explorat\u00f3ria de dados georreferenciados e espaciais usando ferramentas especializadas; REalizar an\u00e1lises de clustering de dados usando ambiente de an\u00e1lise (Python)","title":"Objetivos"},{"location":"#conteudo-programatico","text":"Diferen\u00e7a entre dados espaciais e dados n\u00e3o-espaciais No\u00e7\u00f5es b\u00e1sicas de cartografia e sistemas de coordenadas espaciais Fontes de dados georreferenciados: reposit\u00f3rios, dispositivos m\u00f3veis, IoT Trabalhando com formatos comuns de dados espaciais em bases de dados Geocodifica\u00e7\u00e3o direta e reversa Agrega\u00e7\u00e3o de dados: pontos, linhas, pol\u00edgonos An\u00e1lise explorat\u00f3ria de dados espaciais An\u00e1lise de distribui\u00e7\u00f5es espaciais Autocorrela\u00e7\u00e3o espacial Clusters espaciais","title":"Conte\u00fado Program\u00e1tico"},{"location":"#bibliografia","text":"LAWHEAD, J. Learning Geospatial Analysis with Python: Understand GIS fundamentals and perform remote sensing data analysis using Python. 3.a ed. Packt Publishing, 2019. CRICKARD, P.; VAN REES, E.; TOMS, S. Mastering Geospatial Analysis with Python. 1.a ed. Packt publishing, 2017. CRESSIE, N. Statistics for Spatial Data. 2015.","title":"Bibliografia"},{"location":"teste/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); # Setar o ambiente para uso de Python no Spark import support_functions as sf sf . set_spark_python () # Criar a sessao do Spark from pyspark.sql import SparkSession spark = SparkSession \\ . builder \\ . master ( \"local[2]\" ) \\ . appName ( \"AndreDF\" ) \\ . getOrCreate () spark SparkSession - in-memory SparkContext Spark UI Version v3.0.1 Master local[2] AppName AndreDF sc = spark . sparkContext At\u00e9 agora vimos trabalhando com a estrutura base do Spark, as RDDs. Contudo, essas estruturas s\u00e3o trabalhosas e requerem o tratamento manual de tipos de dados bem como a interpreta\u00e7\u00e3o constante dos dados posicionais na RDD. \u00c9 poss\u00edvel simplificar nosso trabalho com o uso de camadas novas e mais elevadas do Spark. Existe uma s\u00e9rie de fun\u00e7\u00f5es para a cria\u00e7\u00e3o e a cria\u00e7\u00e3o e manipula\u00e7\u00e3o destas novas estruturas de dados. Vamos come\u00e7ar investigando os DataFrames, inspirados na biblioteca Pandas do Python. createDataFrame() \u00b6 Para criar um DataFrame a partir de uma RDD, podemos utilizar a fun\u00e7\u00e3o createDataFrame(RDD) . \u00c9 importante notar que o ponto de entrada das APIs de dados estruturados n\u00e3o \u00e9 mais o sparkContext , mas diretamente a SparkSession acess\u00edvel a partir da nossa vari\u00e1vel spark . dept = [( \"Finance\" , 10 ),( \"Marketing\" , 20 ),( \"Sales\" , 30 ),( \"IT\" , 40 )] rdd = spark . sparkContext . parallelize ( dept ) rdd . collect () [('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)] sdf = spark . createDataFrame ( rdd ) sdf DataFrame[_1: string, _2: bigint] sdf DataFrame[_1: string, _2: bigint] Verificamos que a vari\u00e1vel \u00e9 do tipo DataFrame com colunas com nome _1, _2, e diferentes tipos, j\u00e1 inferidos pelo Spark na cria\u00e7\u00e3o do DataFrame a partir da RDD. take() \u00b6 Podemos ent\u00e3o utilizar a fun\u00e7\u00e3o take para verificar nosso DataFrame. sdf . take ( 5 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20), Row(_1='Sales', _2=30), Row(_1='IT', _2=40)] \u00c9 interessante observar que a fun\u00e7\u00e3o nos mostra uma RDD e n\u00e3o um DataFrame. Essa RDD \u00e9 composta de objetos do tipo Row, ou ent\u00e3o linha. Isso \u00e9, um DataFrame nada mais \u00e9 do que uma RDD onde cada elemento \u00e9 uma linha de uma tabela com suas diferentes colunas. Diferentemente das RDDs padr\u00f5es, contudo, os objetos Row grava consigo os nomes e os tipos das colunas e o Spark mant\u00e9m a gest\u00e3o garantindo que um mesmo DataFrame seja composto de linhas do mesmo tipo. Row() \u00b6 Podemos manualmente criar uma linha, a partir da classe Row. from pyspark.sql import Row Para isso precisamos nomear as colunas e seus valores. Row ( name = 'Alice' , age = 11 ) Row(name='Alice', age=11) Mais tarde entenderemos como concatenar diferentes objetos Row em um DataFrame. asDict() \u00b6 Podemos transformar objetos do tipo Row em dicion\u00e1rios no python. sdf . take ( 2 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)] Para isso, vamos inicialmente guardar duas linhas de nosso DataFrame. rows = sdf . take ( 2 ) Na sequencia utilizamos a fun\u00e7\u00e3o asDict() my_dict = rows [ 0 ] . asDict () my_dict {'_1': 'Finance', '_2': 10} Com isso podemos acessar os valores de cada coluna normalmente como um dicion\u00e1rio. my_dict [ '_1' ] 'Finance' RDD \u00b6 O atributo rdd do DataFrame tamb\u00e9m nos d\u00e1 acesso direto \u00e0 RDD fundamental que constr\u00f3i o DataFrame. sdf . rdd MapPartitionsRDD[33] at javaToPython at NativeMethodAccessorImpl.java:0 Nessa RDD podemos aplicar todas as transforma\u00e7\u00f5es e a\u00e7\u00f5es que aprendemos, desde que respeitando o tipo do elemento Row. sdf . rdd . take ( 2 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)] Acessando o DataFrame \u00b6 A ideia de termos um DataFrame, contudo, \u00e9 n\u00e3o precisarmos operar diretamente nas RDDs. Para isso existe um conjunto de fun\u00e7\u00f5es de acesso direto ao DataFrame. Como estas fun\u00e7\u00f5es operam intrinsecamente em RDDs, elas tamb\u00e9m s\u00e3o transforma\u00e7\u00f5es e a\u00e7\u00f5es. show() \u00b6 A a\u00e7\u00e3o show() nos permite olhar o DataFrame como uma tabela, muito mais pr\u00f3ximo do Pandas DataFrame. Cuidado, contudo, com o desejo de olhar o DataFrame completo. sdf . show () +---------+---+ | _1| _2| +---------+---+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+---+ printSchema() \u00b6 A a\u00e7\u00e3o show() n\u00e3o nos mostra claramente qual \u00e9 o tipo dos dados envolvidos (similar a dtypes). Para isso, podemos imprimir o Schema do DataFrame. O Schema \u00e9 a defini\u00e7\u00e3o do tipo de dado de cada coluna. \u00c9 importante notar que o Spark inferiu esses dados a partir da RDD. O Spark infere a partir do primeiro elemento nos dados. Portanto, \u00e9 importante que na leitura de arquivos, caso a infer\u00eancia seja utilizada, garantir que a primeira linha do arquivo possua o tipo correto. sdf . printSchema () root |-- _1: string (nullable = true) |-- _2: long (nullable = true) dtypes \u00b6 Podemos tamb\u00e9m utilizar o atributo dtypes para mostrar o tipo de cada coluna. sdf . dtypes [('_1', 'string'), ('_2', 'bigint')] Definindo o Schema \u00b6 Idealmente, quando operando em grandes massas de dados, a infer\u00eancia do tipo pela primeira linha n\u00e3o \u00e9 adequada e utilizamos como boa pr\u00e1tica a defini\u00e7\u00e3o manual do schema. Para isso precisamos utilizar os tipos de dados nativos do Spark que s\u00e3o convertidos para e de tipos python pela API pyspark. Para isso importamos os tipos do spark. from pyspark.sql.types import * StructType(), StructField(), IntegerType(), StringType(), FloatType() \u00b6 Cada tipo de vari\u00e1vel \u00e9 na verdade um objeto de uma classe. Ent\u00e3o acessamos estes objetos para instanciar e para identificar os tipos de cada vari\u00e1vel. O Schema de um DataFrame \u00e9 definido como um objeto StructType (similar a uma lista ou tupla) com campos do tipo StructField(). Cada campo (StructField) possui um nome, um tipo (objeto do tipo no spark) e um flag se \u00e9 permitida a exist\u00eancia de nulos. Assim, podemos definir explicitametne um Schema para nosso DataFrame. my_schema = StructType ([ \\ StructField ( 'dept_name' , StringType (), True ), \\ StructField ( 'dept_id' , IntegerType (), True ), \\ ]) Na sequencia, atribu\u00edmos o nosso schema ao DataFrame na sua cria\u00e7\u00e3o. sdf1 = spark . createDataFrame ( rdd , schema = my_schema ) Podemos ent\u00e3o olhar nosso DataFrame e verificar que as colunas agora v\u00eam identificadas. sdf1 . show ( 5 ) +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+-------+ Podemos tamb\u00e9m verificar o Schema do DataFrame com as caracter\u00edsticas que definimos anteriormente. sdf1 . printSchema () root |-- dept_name: string (nullable = true) |-- dept_id: integer (nullable = true) Opera\u00e7\u00f5es b\u00e1sicas com colunas \u00b6 A exemplo do Pandas DataFrame, temos \u00e0 nossa disposi\u00e7\u00e3o uma s\u00e9rie de transforma\u00e7\u00f5es e a\u00e7\u00f5es que nos permite operar sobre o Spark DataFrame. Veremos a seguir as opera\u00e7\u00f5es em colunas. columns \u00b6 O atributo columns retorna, a exemplo do Pandas, uma lista com o nome das colunas. Contudo, diferente do Pandas, n\u00e3o \u00e9 poss\u00edvel sobrescrever este atributo diretamente. sdf . columns ['_1', '_2'] withColumnRenamed() \u00b6 Para renomear uma coluna utilizamos a fun\u00e7\u00e3o withColumnRenamed(). Esta fun\u00e7\u00e3o opera em uma coluna por vez. Assim, podemos utilizar um la\u00e7o para renomear todas as colunas. for old_col , new_col in zip ( sdf . columns , [ 'dept_name' , 'dept_id' ]): sdf = sdf . withColumnRenamed ( old_col , new_col ) sdf . show () +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+-------+ drop() \u00b6 Podemos descartar colunas usando a fun\u00e7\u00e3o .drop() sdf . drop ( 'dept_id' ) . show ( 5 ) +---------+ |dept_name| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+ Acessando colunas \u00b6 Para acessar as colunas de um DataFrame, podemos utilizar a mesma nota\u00e7\u00e3o python. Observe, contudo, que as colunas s\u00e3o representadas como objetos e, assim, o acesso \u00e0 coluna n\u00e3o retorna automaticamente os dados existentes nela. sdf [ 'dept_id' ] Column<b'dept_id'> sdf . dept_id Column<b'dept_id'> select() \u00b6 Para acessar os dados de uma coluna espec\u00edfica, precisamos utilizar a transforma\u00e7\u00e3o select() seguida de uma a\u00e7\u00e3o show(). Esta transforma\u00e7\u00e3o \u00e9 similiar ao SELECT em SQL. sdf . select ( 'dept_name' ) . show ( 5 ) +---------+ |dept_name| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+ Case sensitivity \u00b6 Observe que a exemplo do SQL, DataFrames em Spark n\u00e3o s\u00e3o naturalmente sens\u00edveis ao caso. sdf . select ( 'DEPT_NAME' ) . show ( 5 ) +---------+ |DEPT_NAME| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+ sdf . select ( 'dept_NAME' ) . show ( 5 ) +---------+ |dept_NAME| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+ Opera\u00e7\u00f5es b\u00e1sicas com linhas \u00b6 Da mesma forma que temos opera\u00e7\u00f5es com colunas, temos opera\u00e7\u00f5es com linhas. limit() \u00b6 A transforma\u00e7\u00e3o limit(LIM) nos permite limitar um n\u00famero de registros (linhas no DataFrame) a ser retornado. POdemos concatenar com .collect() e temos o DataFrame no formato de RDD. sdf . limit ( 3 ) . collect () [Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30)] Com isso podemos reduzir o n\u00famero de linhas do nosso DataFrame para aumentar a velocidade do nosso trabalho. sdf = sdf . limit ( 500 ) filter() - filtrando linhas \u00b6 A transforma\u00e7\u00e3o filter() nos permite filtrar linhas com base em condi\u00e7\u00f5es especificadas sobre colunas. sdf . filter ( sdf . dept_id == 20 ) . show () +---------+-------+ |dept_name|dept_id| +---------+-------+ |Marketing| 20| +---------+-------+ where() - filtrando linhas \u00b6 Para manter a similaridade como o SQL, a transforma\u00e7\u00e3o filter() tamb\u00e9m pode ser acessada atrav\u00e9s de seu apelido (alias) where(). sdf . where ( sdf . dept_id == 10 ) . show ( 5 ) +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| +---------+-------+ distinct() - filtrando valores distintos \u00b6 Utilizamos a transforma\u00e7\u00e3o distinct() para selecionar apenas os valores distintos de uma coluna espec\u00edfica. sdf . select ( 'dept_id' ) . distinct () . show () +-------+ |dept_id| +-------+ | 10| | 20| | 30| | 40| +-------+ union() - concatenando linhas \u00b6 Para concatenar linhas no DataFrame, precisamos criar uma RDD com objetos do tipo Row e Schema id\u00eantico. new_rows = [ Row ( dept_name = 'Production' , dept_id = 50 ), Row ( dept_name = 'Human Resources' , dept_id = 60 )] new_rows [Row(dept_name='Production', dept_id=50), Row(dept_name='Human Resources', dept_id=60)] Transformamos essa lista em uma RDD, usando parallelize(), da mesma forma que fizemos anteriormente. parallelizedRows = spark . sparkContext . parallelize ( new_rows ) Por fim, criamos o DataFrame, definindo o Schema. newDF = spark . createDataFrame ( parallelizedRows , my_schema ) newDF . show () +---------------+-------+ | dept_name|dept_id| +---------------+-------+ | Production| 50| |Human Resources| 60| +---------------+-------+ Podemos ent\u00e3o concatenar nosso novo DataFrame ao DataFrame antigo. sdf1 = sdf . union ( newDF ) sdf1 . show () +---------------+-------+ | dept_name|dept_id| +---------------+-------+ | Finance| 10| | Marketing| 20| | Sales| 30| | IT| 40| | Production| 50| |Human Resources| 60| +---------------+-------+ orderBy() - ordenando linhas \u00b6 Com a transforma\u00e7\u00e3o orderBy() podemos ordenar as linhas do DataFrame. Para isso, precisamos utilizar os m\u00e9todos .asc() ou .desc() na coluna chave, indicando a ordem desejada. sdf1 . orderBy ( sdf1 . dept_id . desc ()) . show ( 5 ) +---------------+-------+ | dept_name|dept_id| +---------------+-------+ |Human Resources| 60| | Production| 50| | IT| 40| | Sales| 30| | Marketing| 20| +---------------+-------+ only showing top 5 rows pyspark.sql.functions \u00b6 Existe uma s\u00e9rie de fun\u00e7\u00f5es que facilitam a manipula\u00e7\u00e3o de DataFrames. Elas est\u00e3o dispon\u00edveis no pacote pyspark.sql.functions e podem ser investigadas na refer\u00eancia abaixo: http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions Podemos import\u00e1-las todas atrav\u00e9s da linha de comando abaixo. from pyspark.sql import functions as sf lit() - criando colunas \u00b6 Como visto anteriormente, o Spark possui tipos pr\u00f3prios de dados com correspond\u00eancia com o Python. Se quisermos criar uma coluna constante, podemos evitar o trabalho de criar a coluna em Python e transform\u00e1-la, utilizando a fun\u00e7\u00e3o lit(val) que cria uma coluna de literais (constantes) val. sf . lit ( 10 ) Column<b'10'> withColumn() - adicionando colunas \u00b6 Para isso, precisamos concatenar uma coluna de literais. Isso \u00e9 feito com a transforma\u00e7\u00e3o withColumn(nome, nova_col). sdf1 . withColumn ( 'ones' , sf . lit ( 1 )) . show ( 3 ) +---------+-------+----+ |dept_name|dept_id|ones| +---------+-------+----+ | Finance| 10| 1| |Marketing| 20| 1| | Sales| 30| 1| +---------+-------+----+ only showing top 3 rows col() - acessando colunas \u00b6 Em alguns casos precisamos explicitar que desejamos acessar um objeto do tipo Column. Para isso temos \u00e0 nossa disposi\u00e7\u00e3o a fun\u00e7\u00e3o .col(nome). sdf1 . select ( sf . col ( 'dept_id' )) . show ( 3 ) +-------+ |dept_id| +-------+ | 10| | 20| | 30| +-------+ only showing top 3 rows Observe que temos, com isso, diferentes formas de acessar colunas no select. Em alguns casos espec\u00edficos ser\u00e1 necess\u00e1rio optar pela men\u00e7\u00e3o expl\u00edcita ao tipo do objeto usando .col() sdf1 . select ( sdf1 . dept_id , sf . col ( 'dept_name' ), 'dept_id' ) . show ( 3 ) +-------+---------+-------+ |dept_id|dept_name|dept_id| +-------+---------+-------+ | 10| Finance| 10| | 20|Marketing| 20| | 30| Sales| 30| +-------+---------+-------+ only showing top 3 rows Podemos ainda acessar a RDD resultante da transforma\u00e7\u00e3o. sdf1 . select ( sdf1 . dept_id , sf . col ( 'dept_id' ), 'dept_id' ) . rdd . take ( 3 ) [Row(dept_id=10, dept_id=10, dept_id=10), Row(dept_id=20, dept_id=20, dept_id=20), Row(dept_id=30, dept_id=30, dept_id=30)] alias() \u00b6 A exemplo do SQL, podemos utilizar o m\u00e9todo .alias() de uma coluna espec\u00edfica para renomear colunas rapidamente. sdf . select ( '*' , sf . col ( 'dept_id' ) . alias ( 'identificador' )) . show ( 5 ) +---------+-------+-------------+ |dept_name|dept_id|identificador| +---------+-------+-------------+ | Finance| 10| 10| |Marketing| 20| 20| | Sales| 30| 30| | IT| 40| 40| +---------+-------+-------------+ Express\u00f5es \u00b6 DataFrames s\u00e3o equivalentes de tabelas no Pandas e tamb\u00e9m de tabelas no SQL. Buscando manter compatibilidade com o SQL, express\u00f5es permitem que escrevamos algumas express\u00f5es simplificadas de SQL para operar no DataFrame. expr() \u00b6 Para isso, utilizamos a fun\u00e7\u00e3o expr(). Podemos, por exemplo, renomear colunas. sdf1 . select ( sf . expr ( 'dept_name as departamento' )) . show ( 5 ) +------------+ |departamento| +------------+ | Finance| | Marketing| | Sales| | IT| | Production| +------------+ only showing top 5 rows Podemos tamb\u00e9m operar sobre colunas diretamente, a exemplo do SQL. sdf1 . select ( 'dept_id' , 'dept_name' , sf . expr ( 'dept_id / 10' )) . show ( 3 ) +-------+---------+--------------+ |dept_id|dept_name|(dept_id / 10)| +-------+---------+--------------+ | 10| Finance| 1.0| | 20|Marketing| 2.0| | 30| Sales| 3.0| +-------+---------+--------------+ only showing top 3 rows selectExpr() \u00b6 Como a combina\u00e7\u00e3o de .select() e .expr() \u00e9 muito comum, o spark j\u00e1 nos fornece um atalho: selectExpr(). sdf . selectExpr ( '*' , 'dept_id / 10 as `id by 10`' ) . show ( 5 ) +---------+-------+--------+ |dept_name|dept_id|id by 10| +---------+-------+--------+ | Finance| 10| 1.0| |Marketing| 20| 2.0| | Sales| 30| 3.0| | IT| 40| 4.0| +---------+-------+--------+ Abrindo arquivos CSV \u00b6 At\u00e9 agora trabalhamos com DataFrames criados manualmente a partir de uma RDD pr\u00e9via. Podemos utilizar as fun\u00e7\u00f5es do pacote .read do Spark para ler diretamente fontes de dados. Para carregar arquivos .csv usamos a fun\u00e7\u00e3o .csv. df = spark . read . csv ( '../10_dados/movie_lens/ratings.csv' , header = True ) df DataFrame[userId: string, movieId: string, rating: string, timestamp: string] df . show ( 5 ) +------+-------+------+----------+ |userId|movieId|rating| timestamp| +------+-------+------+----------+ | 1| 296| 5.0|1147880044| | 1| 306| 3.5|1147868817| | 1| 307| 5.0|1147868828| | 1| 665| 5.0|1147878820| | 1| 899| 3.5|1147868510| +------+-------+------+----------+ only showing top 5 rows df . take ( 5 ) [Row(userId='1', movieId='296', rating='5.0', timestamp='1147880044'), Row(userId='1', movieId='306', rating='3.5', timestamp='1147868817'), Row(userId='1', movieId='307', rating='5.0', timestamp='1147868828'), Row(userId='1', movieId='665', rating='5.0', timestamp='1147878820'), Row(userId='1', movieId='899', rating='3.5', timestamp='1147868510')] df . columns ['userId', 'movieId', 'rating', 'timestamp'] df . dtypes [('userId', 'string'), ('movieId', 'string'), ('rating', 'string'), ('timestamp', 'string')] df . printSchema () root |-- userId: string (nullable = true) |-- movieId: string (nullable = true) |-- rating: string (nullable = true) |-- timestamp: string (nullable = true) from pyspark.sql.types import StringType , DoubleType , TimestampType , IntegerType , StructType , StructField labels = (( 'userId' , IntegerType ()), ( 'movieId' , IntegerType ()), ( 'rating' , DoubleType ()), ( 'timestamp' , StringType ()) ) schema = StructType ([ StructField ( x [ 0 ], x [ 1 ], True ) for x in labels ]) print ( schema ) StructType(List(StructField(userId,IntegerType,true),StructField(movieId,IntegerType,true),StructField(rating,DoubleType,true),StructField(timestamp,StringType,true))) df = spark . read . csv ( '../10_dados/movie_lens/ratings.csv' , header = True , schema = schema ) df . printSchema () root |-- userId: integer (nullable = true) |-- movieId: integer (nullable = true) |-- rating: double (nullable = true) |-- timestamp: string (nullable = true) df . show ( 5 ) +------+-------+------+----------+ |userId|movieId|rating| timestamp| +------+-------+------+----------+ | 1| 296| 5.0|1147880044| | 1| 306| 3.5|1147868817| | 1| 307| 5.0|1147868828| | 1| 665| 5.0|1147878820| | 1| 899| 3.5|1147868510| +------+-------+------+----------+ only showing top 5 rows df . select ( 'userId' ) . show ( 5 ) +------+ |userId| +------+ | 1| | 1| | 1| | 1| | 1| +------+ only showing top 5 rows df . select ([ 'movieId' , 'rating' ]) . show ( 5 ) +-------+------+ |movieId|rating| +-------+------+ | 296| 5.0| | 306| 3.5| | 307| 5.0| | 665| 5.0| | 899| 3.5| +-------+------+ only showing top 5 rows from pyspark.sql.functions import lit df = df . withColumn ( 'new_column' , lit ( 1 )) df . show ( 5 ) +------+-------+------+----------+----------+ |userId|movieId|rating| timestamp|new_column| +------+-------+------+----------+----------+ | 1| 296| 5.0|1147880044| 1| | 1| 306| 3.5|1147868817| 1| | 1| 307| 5.0|1147868828| 1| | 1| 665| 5.0|1147878820| 1| | 1| 899| 3.5|1147868510| 1| +------+-------+------+----------+----------+ only showing top 5 rows from pyspark.sql.functions import col , concat df = df . withColumn ( 'concat' , concat ( col ( 'movieId' ), lit ( '-' ), col ( 'rating' ))) df . show ( 5 ) +------+-------+------+----------+----------+-------+ |userId|movieId|rating| timestamp|new_column| concat| +------+-------+------+----------+----------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+----------+-------+ only showing top 5 rows df = df . withColumnRenamed ( 'new_column' , 'constant' ) df . show ( 5 ) +------+-------+------+----------+--------+-------+ |userId|movieId|rating| timestamp|constant| concat| +------+-------+------+----------+--------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+--------+-------+ only showing top 5 rows df . groupBy ( 'movieId' ) . count () . show ( 5 ) +-------+-----+ |movieId|count| +-------+-----+ | 1088|11935| | 1580|40308| | 3175|14659| | 44022| 4833| | 175197| 610| +-------+-----+ only showing top 5 rows df . count () 25000095 df . createOrReplaceTempView ( 'movielens' ) query = spark . sql ( 'select * from movielens limit 5' ) query . explain ( mode = \"formatted\" ) == Physical Plan == CollectLimit (3) +- * Project (2) +- Scan csv (1) (1) Scan csv Output [4]: [userId#191, movieId#192, rating#193, timestamp#194] Batched: false Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv] ReadSchema: struct<userId:int,movieId:int,rating:double,timestamp:string> (2) Project [codegen id : 1] Output [6]: [userId#191, movieId#192, rating#193, timestamp#194, 1 AS constant#334, concat(cast(movieId#192 as string), -, cast(rating#193 as string)) AS concat#298] Input [4]: [userId#191, movieId#192, rating#193, timestamp#194] (3) CollectLimit Input [6]: [userId#191, movieId#192, rating#193, timestamp#194, constant#334, concat#298] Arguments: 5 query . show ( 5 ) +------+-------+------+----------+--------+-------+ |userId|movieId|rating| timestamp|constant| concat| +------+-------+------+----------+--------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+--------+-------+ query = spark . sql ( \"\"\"select movieId, count(*) from movielens where rating = 5 group by movieId order by 2 desc limit 5\"\"\" ) query . explain ( mode = \"formatted\" ) == Physical Plan == TakeOrderedAndProject (7) +- * HashAggregate (6) +- Exchange (5) +- * HashAggregate (4) +- * Project (3) +- * Filter (2) +- Scan csv (1) (1) Scan csv Output [2]: [movieId#192, rating#193] Batched: false Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv] PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)] ReadSchema: struct<movieId:int,rating:double> (2) Filter [codegen id : 1] Input [2]: [movieId#192, rating#193] Condition : (isnotnull(rating#193) AND (rating#193 = 5.0)) (3) Project [codegen id : 1] Output [1]: [movieId#192] Input [2]: [movieId#192, rating#193] (4) HashAggregate [codegen id : 1] Input [1]: [movieId#192] Keys [1]: [movieId#192] Functions [1]: [partial_count(1)] Aggregate Attributes [1]: [count#463L] Results [2]: [movieId#192, count#464L] (5) Exchange Input [2]: [movieId#192, count#464L] Arguments: hashpartitioning(movieId#192, 200), true, [id=#334] (6) HashAggregate [codegen id : 2] Input [2]: [movieId#192, count#464L] Keys [1]: [movieId#192] Functions [1]: [count(1)] Aggregate Attributes [1]: [count(1)#458L] Results [2]: [movieId#192, count(1)#458L AS count(1)#459L] (7) TakeOrderedAndProject Input [2]: [movieId#192, count(1)#459L] Arguments: 5, [count(1)#459L DESC NULLS LAST], [movieId#192, count(1)#459L] query . show ( 5 ) +-------+--------+ |movieId|count(1)| +-------+--------+ | 318| 39553| | 296| 32169| | 356| 25918| | 260| 25804| | 2571| 25482| +-------+--------+ spark . conf . set ( \"spark.sql.repl.eagerEval.enabled\" , True ) query . cache () movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query . explain () == Physical Plan == TakeOrderedAndProject(limit=5, orderBy=[count(1)#459L DESC NULLS LAST], output=[movieId#192,count(1)#459L]) +- *(2) HashAggregate(keys=[movieId#192], functions=[count(1)]) +- Exchange hashpartitioning(movieId#192, 200), true, [id=#334] +- *(1) HashAggregate(keys=[movieId#192], functions=[partial_count(1)]) +- *(1) Project [movieId#192] +- *(1) Filter (isnotnull(rating#193) AND (rating#193 = 5.0)) +- FileScan csv [movieId#192,rating#193] Batched: false, DataFilters: [isnotnull(rating#193), (rating#193 = 5.0)], Format: CSV, Location: InMemoryFileIndex[file:/data/aluno/shared/10_dados/movie_lens/ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)], ReadSchema: struct<movieId:int,rating:double> query . unpersist () movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482","title":"Teste"},{"location":"teste/#createdataframe","text":"Para criar um DataFrame a partir de uma RDD, podemos utilizar a fun\u00e7\u00e3o createDataFrame(RDD) . \u00c9 importante notar que o ponto de entrada das APIs de dados estruturados n\u00e3o \u00e9 mais o sparkContext , mas diretamente a SparkSession acess\u00edvel a partir da nossa vari\u00e1vel spark . dept = [( \"Finance\" , 10 ),( \"Marketing\" , 20 ),( \"Sales\" , 30 ),( \"IT\" , 40 )] rdd = spark . sparkContext . parallelize ( dept ) rdd . collect () [('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)] sdf = spark . createDataFrame ( rdd ) sdf DataFrame[_1: string, _2: bigint] sdf DataFrame[_1: string, _2: bigint] Verificamos que a vari\u00e1vel \u00e9 do tipo DataFrame com colunas com nome _1, _2, e diferentes tipos, j\u00e1 inferidos pelo Spark na cria\u00e7\u00e3o do DataFrame a partir da RDD.","title":"createDataFrame()"},{"location":"teste/#take","text":"Podemos ent\u00e3o utilizar a fun\u00e7\u00e3o take para verificar nosso DataFrame. sdf . take ( 5 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20), Row(_1='Sales', _2=30), Row(_1='IT', _2=40)] \u00c9 interessante observar que a fun\u00e7\u00e3o nos mostra uma RDD e n\u00e3o um DataFrame. Essa RDD \u00e9 composta de objetos do tipo Row, ou ent\u00e3o linha. Isso \u00e9, um DataFrame nada mais \u00e9 do que uma RDD onde cada elemento \u00e9 uma linha de uma tabela com suas diferentes colunas. Diferentemente das RDDs padr\u00f5es, contudo, os objetos Row grava consigo os nomes e os tipos das colunas e o Spark mant\u00e9m a gest\u00e3o garantindo que um mesmo DataFrame seja composto de linhas do mesmo tipo.","title":"take()"},{"location":"teste/#row","text":"Podemos manualmente criar uma linha, a partir da classe Row. from pyspark.sql import Row Para isso precisamos nomear as colunas e seus valores. Row ( name = 'Alice' , age = 11 ) Row(name='Alice', age=11) Mais tarde entenderemos como concatenar diferentes objetos Row em um DataFrame.","title":"Row()"},{"location":"teste/#asdict","text":"Podemos transformar objetos do tipo Row em dicion\u00e1rios no python. sdf . take ( 2 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)] Para isso, vamos inicialmente guardar duas linhas de nosso DataFrame. rows = sdf . take ( 2 ) Na sequencia utilizamos a fun\u00e7\u00e3o asDict() my_dict = rows [ 0 ] . asDict () my_dict {'_1': 'Finance', '_2': 10} Com isso podemos acessar os valores de cada coluna normalmente como um dicion\u00e1rio. my_dict [ '_1' ] 'Finance'","title":"asDict()"},{"location":"teste/#rdd","text":"O atributo rdd do DataFrame tamb\u00e9m nos d\u00e1 acesso direto \u00e0 RDD fundamental que constr\u00f3i o DataFrame. sdf . rdd MapPartitionsRDD[33] at javaToPython at NativeMethodAccessorImpl.java:0 Nessa RDD podemos aplicar todas as transforma\u00e7\u00f5es e a\u00e7\u00f5es que aprendemos, desde que respeitando o tipo do elemento Row. sdf . rdd . take ( 2 ) [Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)]","title":"RDD"},{"location":"teste/#acessando-o-dataframe","text":"A ideia de termos um DataFrame, contudo, \u00e9 n\u00e3o precisarmos operar diretamente nas RDDs. Para isso existe um conjunto de fun\u00e7\u00f5es de acesso direto ao DataFrame. Como estas fun\u00e7\u00f5es operam intrinsecamente em RDDs, elas tamb\u00e9m s\u00e3o transforma\u00e7\u00f5es e a\u00e7\u00f5es.","title":"Acessando o DataFrame"},{"location":"teste/#show","text":"A a\u00e7\u00e3o show() nos permite olhar o DataFrame como uma tabela, muito mais pr\u00f3ximo do Pandas DataFrame. Cuidado, contudo, com o desejo de olhar o DataFrame completo. sdf . show () +---------+---+ | _1| _2| +---------+---+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+---+","title":"show()"},{"location":"teste/#printschema","text":"A a\u00e7\u00e3o show() n\u00e3o nos mostra claramente qual \u00e9 o tipo dos dados envolvidos (similar a dtypes). Para isso, podemos imprimir o Schema do DataFrame. O Schema \u00e9 a defini\u00e7\u00e3o do tipo de dado de cada coluna. \u00c9 importante notar que o Spark inferiu esses dados a partir da RDD. O Spark infere a partir do primeiro elemento nos dados. Portanto, \u00e9 importante que na leitura de arquivos, caso a infer\u00eancia seja utilizada, garantir que a primeira linha do arquivo possua o tipo correto. sdf . printSchema () root |-- _1: string (nullable = true) |-- _2: long (nullable = true)","title":"printSchema()"},{"location":"teste/#dtypes","text":"Podemos tamb\u00e9m utilizar o atributo dtypes para mostrar o tipo de cada coluna. sdf . dtypes [('_1', 'string'), ('_2', 'bigint')]","title":"dtypes"},{"location":"teste/#definindo-o-schema","text":"Idealmente, quando operando em grandes massas de dados, a infer\u00eancia do tipo pela primeira linha n\u00e3o \u00e9 adequada e utilizamos como boa pr\u00e1tica a defini\u00e7\u00e3o manual do schema. Para isso precisamos utilizar os tipos de dados nativos do Spark que s\u00e3o convertidos para e de tipos python pela API pyspark. Para isso importamos os tipos do spark. from pyspark.sql.types import *","title":"Definindo o Schema"},{"location":"teste/#structtype-structfield-integertype-stringtype-floattype","text":"Cada tipo de vari\u00e1vel \u00e9 na verdade um objeto de uma classe. Ent\u00e3o acessamos estes objetos para instanciar e para identificar os tipos de cada vari\u00e1vel. O Schema de um DataFrame \u00e9 definido como um objeto StructType (similar a uma lista ou tupla) com campos do tipo StructField(). Cada campo (StructField) possui um nome, um tipo (objeto do tipo no spark) e um flag se \u00e9 permitida a exist\u00eancia de nulos. Assim, podemos definir explicitametne um Schema para nosso DataFrame. my_schema = StructType ([ \\ StructField ( 'dept_name' , StringType (), True ), \\ StructField ( 'dept_id' , IntegerType (), True ), \\ ]) Na sequencia, atribu\u00edmos o nosso schema ao DataFrame na sua cria\u00e7\u00e3o. sdf1 = spark . createDataFrame ( rdd , schema = my_schema ) Podemos ent\u00e3o olhar nosso DataFrame e verificar que as colunas agora v\u00eam identificadas. sdf1 . show ( 5 ) +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+-------+ Podemos tamb\u00e9m verificar o Schema do DataFrame com as caracter\u00edsticas que definimos anteriormente. sdf1 . printSchema () root |-- dept_name: string (nullable = true) |-- dept_id: integer (nullable = true)","title":"StructType(), StructField(), IntegerType(), StringType(), FloatType()"},{"location":"teste/#operacoes-basicas-com-colunas","text":"A exemplo do Pandas DataFrame, temos \u00e0 nossa disposi\u00e7\u00e3o uma s\u00e9rie de transforma\u00e7\u00f5es e a\u00e7\u00f5es que nos permite operar sobre o Spark DataFrame. Veremos a seguir as opera\u00e7\u00f5es em colunas.","title":"Opera\u00e7\u00f5es b\u00e1sicas com colunas"},{"location":"teste/#columns","text":"O atributo columns retorna, a exemplo do Pandas, uma lista com o nome das colunas. Contudo, diferente do Pandas, n\u00e3o \u00e9 poss\u00edvel sobrescrever este atributo diretamente. sdf . columns ['_1', '_2']","title":"columns"},{"location":"teste/#withcolumnrenamed","text":"Para renomear uma coluna utilizamos a fun\u00e7\u00e3o withColumnRenamed(). Esta fun\u00e7\u00e3o opera em uma coluna por vez. Assim, podemos utilizar um la\u00e7o para renomear todas as colunas. for old_col , new_col in zip ( sdf . columns , [ 'dept_name' , 'dept_id' ]): sdf = sdf . withColumnRenamed ( old_col , new_col ) sdf . show () +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| |Marketing| 20| | Sales| 30| | IT| 40| +---------+-------+","title":"withColumnRenamed()"},{"location":"teste/#drop","text":"Podemos descartar colunas usando a fun\u00e7\u00e3o .drop() sdf . drop ( 'dept_id' ) . show ( 5 ) +---------+ |dept_name| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+","title":"drop()"},{"location":"teste/#acessando-colunas","text":"Para acessar as colunas de um DataFrame, podemos utilizar a mesma nota\u00e7\u00e3o python. Observe, contudo, que as colunas s\u00e3o representadas como objetos e, assim, o acesso \u00e0 coluna n\u00e3o retorna automaticamente os dados existentes nela. sdf [ 'dept_id' ] Column<b'dept_id'> sdf . dept_id Column<b'dept_id'>","title":"Acessando colunas"},{"location":"teste/#select","text":"Para acessar os dados de uma coluna espec\u00edfica, precisamos utilizar a transforma\u00e7\u00e3o select() seguida de uma a\u00e7\u00e3o show(). Esta transforma\u00e7\u00e3o \u00e9 similiar ao SELECT em SQL. sdf . select ( 'dept_name' ) . show ( 5 ) +---------+ |dept_name| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+","title":"select()"},{"location":"teste/#case-sensitivity","text":"Observe que a exemplo do SQL, DataFrames em Spark n\u00e3o s\u00e3o naturalmente sens\u00edveis ao caso. sdf . select ( 'DEPT_NAME' ) . show ( 5 ) +---------+ |DEPT_NAME| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+ sdf . select ( 'dept_NAME' ) . show ( 5 ) +---------+ |dept_NAME| +---------+ | Finance| |Marketing| | Sales| | IT| +---------+","title":"Case sensitivity"},{"location":"teste/#operacoes-basicas-com-linhas","text":"Da mesma forma que temos opera\u00e7\u00f5es com colunas, temos opera\u00e7\u00f5es com linhas.","title":"Opera\u00e7\u00f5es b\u00e1sicas com linhas"},{"location":"teste/#limit","text":"A transforma\u00e7\u00e3o limit(LIM) nos permite limitar um n\u00famero de registros (linhas no DataFrame) a ser retornado. POdemos concatenar com .collect() e temos o DataFrame no formato de RDD. sdf . limit ( 3 ) . collect () [Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30)] Com isso podemos reduzir o n\u00famero de linhas do nosso DataFrame para aumentar a velocidade do nosso trabalho. sdf = sdf . limit ( 500 )","title":"limit()"},{"location":"teste/#filter-filtrando-linhas","text":"A transforma\u00e7\u00e3o filter() nos permite filtrar linhas com base em condi\u00e7\u00f5es especificadas sobre colunas. sdf . filter ( sdf . dept_id == 20 ) . show () +---------+-------+ |dept_name|dept_id| +---------+-------+ |Marketing| 20| +---------+-------+","title":"filter() - filtrando linhas"},{"location":"teste/#where-filtrando-linhas","text":"Para manter a similaridade como o SQL, a transforma\u00e7\u00e3o filter() tamb\u00e9m pode ser acessada atrav\u00e9s de seu apelido (alias) where(). sdf . where ( sdf . dept_id == 10 ) . show ( 5 ) +---------+-------+ |dept_name|dept_id| +---------+-------+ | Finance| 10| +---------+-------+","title":"where() - filtrando linhas"},{"location":"teste/#distinct-filtrando-valores-distintos","text":"Utilizamos a transforma\u00e7\u00e3o distinct() para selecionar apenas os valores distintos de uma coluna espec\u00edfica. sdf . select ( 'dept_id' ) . distinct () . show () +-------+ |dept_id| +-------+ | 10| | 20| | 30| | 40| +-------+","title":"distinct() - filtrando valores distintos"},{"location":"teste/#union-concatenando-linhas","text":"Para concatenar linhas no DataFrame, precisamos criar uma RDD com objetos do tipo Row e Schema id\u00eantico. new_rows = [ Row ( dept_name = 'Production' , dept_id = 50 ), Row ( dept_name = 'Human Resources' , dept_id = 60 )] new_rows [Row(dept_name='Production', dept_id=50), Row(dept_name='Human Resources', dept_id=60)] Transformamos essa lista em uma RDD, usando parallelize(), da mesma forma que fizemos anteriormente. parallelizedRows = spark . sparkContext . parallelize ( new_rows ) Por fim, criamos o DataFrame, definindo o Schema. newDF = spark . createDataFrame ( parallelizedRows , my_schema ) newDF . show () +---------------+-------+ | dept_name|dept_id| +---------------+-------+ | Production| 50| |Human Resources| 60| +---------------+-------+ Podemos ent\u00e3o concatenar nosso novo DataFrame ao DataFrame antigo. sdf1 = sdf . union ( newDF ) sdf1 . show () +---------------+-------+ | dept_name|dept_id| +---------------+-------+ | Finance| 10| | Marketing| 20| | Sales| 30| | IT| 40| | Production| 50| |Human Resources| 60| +---------------+-------+","title":"union() - concatenando linhas"},{"location":"teste/#orderby-ordenando-linhas","text":"Com a transforma\u00e7\u00e3o orderBy() podemos ordenar as linhas do DataFrame. Para isso, precisamos utilizar os m\u00e9todos .asc() ou .desc() na coluna chave, indicando a ordem desejada. sdf1 . orderBy ( sdf1 . dept_id . desc ()) . show ( 5 ) +---------------+-------+ | dept_name|dept_id| +---------------+-------+ |Human Resources| 60| | Production| 50| | IT| 40| | Sales| 30| | Marketing| 20| +---------------+-------+ only showing top 5 rows","title":"orderBy() - ordenando linhas"},{"location":"teste/#pysparksqlfunctions","text":"Existe uma s\u00e9rie de fun\u00e7\u00f5es que facilitam a manipula\u00e7\u00e3o de DataFrames. Elas est\u00e3o dispon\u00edveis no pacote pyspark.sql.functions e podem ser investigadas na refer\u00eancia abaixo: http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions Podemos import\u00e1-las todas atrav\u00e9s da linha de comando abaixo. from pyspark.sql import functions as sf","title":"pyspark.sql.functions"},{"location":"teste/#lit-criando-colunas","text":"Como visto anteriormente, o Spark possui tipos pr\u00f3prios de dados com correspond\u00eancia com o Python. Se quisermos criar uma coluna constante, podemos evitar o trabalho de criar a coluna em Python e transform\u00e1-la, utilizando a fun\u00e7\u00e3o lit(val) que cria uma coluna de literais (constantes) val. sf . lit ( 10 ) Column<b'10'>","title":"lit() - criando colunas"},{"location":"teste/#withcolumn-adicionando-colunas","text":"Para isso, precisamos concatenar uma coluna de literais. Isso \u00e9 feito com a transforma\u00e7\u00e3o withColumn(nome, nova_col). sdf1 . withColumn ( 'ones' , sf . lit ( 1 )) . show ( 3 ) +---------+-------+----+ |dept_name|dept_id|ones| +---------+-------+----+ | Finance| 10| 1| |Marketing| 20| 1| | Sales| 30| 1| +---------+-------+----+ only showing top 3 rows","title":"withColumn() - adicionando colunas"},{"location":"teste/#col-acessando-colunas","text":"Em alguns casos precisamos explicitar que desejamos acessar um objeto do tipo Column. Para isso temos \u00e0 nossa disposi\u00e7\u00e3o a fun\u00e7\u00e3o .col(nome). sdf1 . select ( sf . col ( 'dept_id' )) . show ( 3 ) +-------+ |dept_id| +-------+ | 10| | 20| | 30| +-------+ only showing top 3 rows Observe que temos, com isso, diferentes formas de acessar colunas no select. Em alguns casos espec\u00edficos ser\u00e1 necess\u00e1rio optar pela men\u00e7\u00e3o expl\u00edcita ao tipo do objeto usando .col() sdf1 . select ( sdf1 . dept_id , sf . col ( 'dept_name' ), 'dept_id' ) . show ( 3 ) +-------+---------+-------+ |dept_id|dept_name|dept_id| +-------+---------+-------+ | 10| Finance| 10| | 20|Marketing| 20| | 30| Sales| 30| +-------+---------+-------+ only showing top 3 rows Podemos ainda acessar a RDD resultante da transforma\u00e7\u00e3o. sdf1 . select ( sdf1 . dept_id , sf . col ( 'dept_id' ), 'dept_id' ) . rdd . take ( 3 ) [Row(dept_id=10, dept_id=10, dept_id=10), Row(dept_id=20, dept_id=20, dept_id=20), Row(dept_id=30, dept_id=30, dept_id=30)]","title":"col() - acessando colunas"},{"location":"teste/#alias","text":"A exemplo do SQL, podemos utilizar o m\u00e9todo .alias() de uma coluna espec\u00edfica para renomear colunas rapidamente. sdf . select ( '*' , sf . col ( 'dept_id' ) . alias ( 'identificador' )) . show ( 5 ) +---------+-------+-------------+ |dept_name|dept_id|identificador| +---------+-------+-------------+ | Finance| 10| 10| |Marketing| 20| 20| | Sales| 30| 30| | IT| 40| 40| +---------+-------+-------------+","title":"alias()"},{"location":"teste/#expressoes","text":"DataFrames s\u00e3o equivalentes de tabelas no Pandas e tamb\u00e9m de tabelas no SQL. Buscando manter compatibilidade com o SQL, express\u00f5es permitem que escrevamos algumas express\u00f5es simplificadas de SQL para operar no DataFrame.","title":"Express\u00f5es"},{"location":"teste/#expr","text":"Para isso, utilizamos a fun\u00e7\u00e3o expr(). Podemos, por exemplo, renomear colunas. sdf1 . select ( sf . expr ( 'dept_name as departamento' )) . show ( 5 ) +------------+ |departamento| +------------+ | Finance| | Marketing| | Sales| | IT| | Production| +------------+ only showing top 5 rows Podemos tamb\u00e9m operar sobre colunas diretamente, a exemplo do SQL. sdf1 . select ( 'dept_id' , 'dept_name' , sf . expr ( 'dept_id / 10' )) . show ( 3 ) +-------+---------+--------------+ |dept_id|dept_name|(dept_id / 10)| +-------+---------+--------------+ | 10| Finance| 1.0| | 20|Marketing| 2.0| | 30| Sales| 3.0| +-------+---------+--------------+ only showing top 3 rows","title":"expr()"},{"location":"teste/#selectexpr","text":"Como a combina\u00e7\u00e3o de .select() e .expr() \u00e9 muito comum, o spark j\u00e1 nos fornece um atalho: selectExpr(). sdf . selectExpr ( '*' , 'dept_id / 10 as `id by 10`' ) . show ( 5 ) +---------+-------+--------+ |dept_name|dept_id|id by 10| +---------+-------+--------+ | Finance| 10| 1.0| |Marketing| 20| 2.0| | Sales| 30| 3.0| | IT| 40| 4.0| +---------+-------+--------+","title":"selectExpr()"},{"location":"teste/#abrindo-arquivos-csv","text":"At\u00e9 agora trabalhamos com DataFrames criados manualmente a partir de uma RDD pr\u00e9via. Podemos utilizar as fun\u00e7\u00f5es do pacote .read do Spark para ler diretamente fontes de dados. Para carregar arquivos .csv usamos a fun\u00e7\u00e3o .csv. df = spark . read . csv ( '../10_dados/movie_lens/ratings.csv' , header = True ) df DataFrame[userId: string, movieId: string, rating: string, timestamp: string] df . show ( 5 ) +------+-------+------+----------+ |userId|movieId|rating| timestamp| +------+-------+------+----------+ | 1| 296| 5.0|1147880044| | 1| 306| 3.5|1147868817| | 1| 307| 5.0|1147868828| | 1| 665| 5.0|1147878820| | 1| 899| 3.5|1147868510| +------+-------+------+----------+ only showing top 5 rows df . take ( 5 ) [Row(userId='1', movieId='296', rating='5.0', timestamp='1147880044'), Row(userId='1', movieId='306', rating='3.5', timestamp='1147868817'), Row(userId='1', movieId='307', rating='5.0', timestamp='1147868828'), Row(userId='1', movieId='665', rating='5.0', timestamp='1147878820'), Row(userId='1', movieId='899', rating='3.5', timestamp='1147868510')] df . columns ['userId', 'movieId', 'rating', 'timestamp'] df . dtypes [('userId', 'string'), ('movieId', 'string'), ('rating', 'string'), ('timestamp', 'string')] df . printSchema () root |-- userId: string (nullable = true) |-- movieId: string (nullable = true) |-- rating: string (nullable = true) |-- timestamp: string (nullable = true) from pyspark.sql.types import StringType , DoubleType , TimestampType , IntegerType , StructType , StructField labels = (( 'userId' , IntegerType ()), ( 'movieId' , IntegerType ()), ( 'rating' , DoubleType ()), ( 'timestamp' , StringType ()) ) schema = StructType ([ StructField ( x [ 0 ], x [ 1 ], True ) for x in labels ]) print ( schema ) StructType(List(StructField(userId,IntegerType,true),StructField(movieId,IntegerType,true),StructField(rating,DoubleType,true),StructField(timestamp,StringType,true))) df = spark . read . csv ( '../10_dados/movie_lens/ratings.csv' , header = True , schema = schema ) df . printSchema () root |-- userId: integer (nullable = true) |-- movieId: integer (nullable = true) |-- rating: double (nullable = true) |-- timestamp: string (nullable = true) df . show ( 5 ) +------+-------+------+----------+ |userId|movieId|rating| timestamp| +------+-------+------+----------+ | 1| 296| 5.0|1147880044| | 1| 306| 3.5|1147868817| | 1| 307| 5.0|1147868828| | 1| 665| 5.0|1147878820| | 1| 899| 3.5|1147868510| +------+-------+------+----------+ only showing top 5 rows df . select ( 'userId' ) . show ( 5 ) +------+ |userId| +------+ | 1| | 1| | 1| | 1| | 1| +------+ only showing top 5 rows df . select ([ 'movieId' , 'rating' ]) . show ( 5 ) +-------+------+ |movieId|rating| +-------+------+ | 296| 5.0| | 306| 3.5| | 307| 5.0| | 665| 5.0| | 899| 3.5| +-------+------+ only showing top 5 rows from pyspark.sql.functions import lit df = df . withColumn ( 'new_column' , lit ( 1 )) df . show ( 5 ) +------+-------+------+----------+----------+ |userId|movieId|rating| timestamp|new_column| +------+-------+------+----------+----------+ | 1| 296| 5.0|1147880044| 1| | 1| 306| 3.5|1147868817| 1| | 1| 307| 5.0|1147868828| 1| | 1| 665| 5.0|1147878820| 1| | 1| 899| 3.5|1147868510| 1| +------+-------+------+----------+----------+ only showing top 5 rows from pyspark.sql.functions import col , concat df = df . withColumn ( 'concat' , concat ( col ( 'movieId' ), lit ( '-' ), col ( 'rating' ))) df . show ( 5 ) +------+-------+------+----------+----------+-------+ |userId|movieId|rating| timestamp|new_column| concat| +------+-------+------+----------+----------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+----------+-------+ only showing top 5 rows df = df . withColumnRenamed ( 'new_column' , 'constant' ) df . show ( 5 ) +------+-------+------+----------+--------+-------+ |userId|movieId|rating| timestamp|constant| concat| +------+-------+------+----------+--------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+--------+-------+ only showing top 5 rows df . groupBy ( 'movieId' ) . count () . show ( 5 ) +-------+-----+ |movieId|count| +-------+-----+ | 1088|11935| | 1580|40308| | 3175|14659| | 44022| 4833| | 175197| 610| +-------+-----+ only showing top 5 rows df . count () 25000095 df . createOrReplaceTempView ( 'movielens' ) query = spark . sql ( 'select * from movielens limit 5' ) query . explain ( mode = \"formatted\" ) == Physical Plan == CollectLimit (3) +- * Project (2) +- Scan csv (1) (1) Scan csv Output [4]: [userId#191, movieId#192, rating#193, timestamp#194] Batched: false Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv] ReadSchema: struct<userId:int,movieId:int,rating:double,timestamp:string> (2) Project [codegen id : 1] Output [6]: [userId#191, movieId#192, rating#193, timestamp#194, 1 AS constant#334, concat(cast(movieId#192 as string), -, cast(rating#193 as string)) AS concat#298] Input [4]: [userId#191, movieId#192, rating#193, timestamp#194] (3) CollectLimit Input [6]: [userId#191, movieId#192, rating#193, timestamp#194, constant#334, concat#298] Arguments: 5 query . show ( 5 ) +------+-------+------+----------+--------+-------+ |userId|movieId|rating| timestamp|constant| concat| +------+-------+------+----------+--------+-------+ | 1| 296| 5.0|1147880044| 1|296-5.0| | 1| 306| 3.5|1147868817| 1|306-3.5| | 1| 307| 5.0|1147868828| 1|307-5.0| | 1| 665| 5.0|1147878820| 1|665-5.0| | 1| 899| 3.5|1147868510| 1|899-3.5| +------+-------+------+----------+--------+-------+ query = spark . sql ( \"\"\"select movieId, count(*) from movielens where rating = 5 group by movieId order by 2 desc limit 5\"\"\" ) query . explain ( mode = \"formatted\" ) == Physical Plan == TakeOrderedAndProject (7) +- * HashAggregate (6) +- Exchange (5) +- * HashAggregate (4) +- * Project (3) +- * Filter (2) +- Scan csv (1) (1) Scan csv Output [2]: [movieId#192, rating#193] Batched: false Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv] PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)] ReadSchema: struct<movieId:int,rating:double> (2) Filter [codegen id : 1] Input [2]: [movieId#192, rating#193] Condition : (isnotnull(rating#193) AND (rating#193 = 5.0)) (3) Project [codegen id : 1] Output [1]: [movieId#192] Input [2]: [movieId#192, rating#193] (4) HashAggregate [codegen id : 1] Input [1]: [movieId#192] Keys [1]: [movieId#192] Functions [1]: [partial_count(1)] Aggregate Attributes [1]: [count#463L] Results [2]: [movieId#192, count#464L] (5) Exchange Input [2]: [movieId#192, count#464L] Arguments: hashpartitioning(movieId#192, 200), true, [id=#334] (6) HashAggregate [codegen id : 2] Input [2]: [movieId#192, count#464L] Keys [1]: [movieId#192] Functions [1]: [count(1)] Aggregate Attributes [1]: [count(1)#458L] Results [2]: [movieId#192, count(1)#458L AS count(1)#459L] (7) TakeOrderedAndProject Input [2]: [movieId#192, count(1)#459L] Arguments: 5, [count(1)#459L DESC NULLS LAST], [movieId#192, count(1)#459L] query . show ( 5 ) +-------+--------+ |movieId|count(1)| +-------+--------+ | 318| 39553| | 296| 32169| | 356| 25918| | 260| 25804| | 2571| 25482| +-------+--------+ spark . conf . set ( \"spark.sql.repl.eagerEval.enabled\" , True ) query . cache () movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query . explain () == Physical Plan == TakeOrderedAndProject(limit=5, orderBy=[count(1)#459L DESC NULLS LAST], output=[movieId#192,count(1)#459L]) +- *(2) HashAggregate(keys=[movieId#192], functions=[count(1)]) +- Exchange hashpartitioning(movieId#192, 200), true, [id=#334] +- *(1) HashAggregate(keys=[movieId#192], functions=[partial_count(1)]) +- *(1) Project [movieId#192] +- *(1) Filter (isnotnull(rating#193) AND (rating#193 = 5.0)) +- FileScan csv [movieId#192,rating#193] Batched: false, DataFilters: [isnotnull(rating#193), (rating#193 = 5.0)], Format: CSV, Location: InMemoryFileIndex[file:/data/aluno/shared/10_dados/movie_lens/ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)], ReadSchema: struct<movieId:int,rating:double> query . unpersist () movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482 query movieId count(1) 318 39553 296 32169 356 25918 260 25804 2571 25482","title":"Abrindo arquivos CSV"}]}