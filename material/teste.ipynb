{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:23.734268Z",
     "start_time": "2021-05-21T00:17:16.123247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setar o ambiente para uso de Python no Spark\n",
    "import support_functions as sf\n",
    "sf.set_spark_python()\n",
    "\n",
    "# Criar a sessao do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"AndreDF\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:26.088238Z",
     "start_time": "2021-05-21T00:17:26.064592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-39-225.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AndreDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffa59d92a60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:27.899075Z",
     "start_time": "2021-05-21T00:17:27.891439Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora vimos trabalhando com a estrutura base do Spark, as RDDs. Contudo, essas estruturas são trabalhosas e requerem o tratamento manual de tipos de dados bem como a interpretação constante dos dados posicionais na RDD.\n",
    "\n",
    "É possível simplificar nosso trabalho com o uso de camadas novas e mais elevadas do Spark.\n",
    "\n",
    "\n",
    "Existe uma série de funções para a criação e a criação e manipulação destas novas estruturas de dados. Vamos começar investigando os DataFrames, inspirados na biblioteca Pandas do Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## createDataFrame()\n",
    "Para criar um DataFrame a partir de uma RDD, podemos utilizar a função `createDataFrame(RDD)`. É importante notar que o ponto de entrada das APIs de dados estruturados não é mais o `sparkContext`, mas diretamente a `SparkSession` acessível a partir da nossa variável `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:20:53.995743Z",
     "start_time": "2021-05-21T00:20:53.960306Z"
    }
   },
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:20:59.480156Z",
     "start_time": "2021-05-21T00:20:59.344274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:21:19.780167Z",
     "start_time": "2021-05-21T00:21:18.710684Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:21:22.105019Z",
     "start_time": "2021-05-21T00:21:22.095313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:22:41.521281Z",
     "start_time": "2021-05-21T00:22:41.510506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que a variável é do tipo DataFrame com colunas com nome `_1, _2,` e diferentes tipos, já __inferidos__ pelo Spark na criação do DataFrame a partir da RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take()\n",
    "Podemos então utilizar a função take para verificar nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:23:11.797527Z",
     "start_time": "2021-05-21T00:23:11.583109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Finance', _2=10),\n",
       " Row(_1='Marketing', _2=20),\n",
       " Row(_1='Sales', _2=30),\n",
       " Row(_1='IT', _2=40)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É interessante observar que a função nos mostra uma RDD e não um DataFrame. Essa RDD é composta de objetos do tipo Row, ou então linha. Isso é, um DataFrame nada mais é do que uma RDD onde cada elemento é uma linha de uma tabela com suas diferentes colunas. \n",
    "\n",
    "Diferentemente das RDDs padrões, contudo, os objetos Row grava consigo os nomes e os tipos das colunas e o Spark mantém a gestão garantindo que um mesmo DataFrame seja composto de linhas do mesmo tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row()\n",
    "\n",
    "Podemos manualmente criar uma linha, a partir da classe Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:24:12.314546Z",
     "start_time": "2021-05-21T00:24:12.310669Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso precisamos nomear as colunas e seus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:24:33.556002Z",
     "start_time": "2021-05-21T00:24:33.550280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Alice', age=11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Row(name = 'Alice', age = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais tarde entenderemos como concatenar diferentes objetos Row em um DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asDict()\n",
    "\n",
    "Podemos transformar objetos do tipo Row em dicionários no python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:04.122126Z",
     "start_time": "2021-05-21T00:25:04.020307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, vamos inicialmente guardar duas linhas de nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:20.921552Z",
     "start_time": "2021-05-21T00:25:20.820444Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = sdf.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequencia utilizamos a função asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:45.766270Z",
     "start_time": "2021-05-21T00:25:45.760421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_1': 'Finance', '_2': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = rows[0].asDict()\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso podemos acessar os valores de cada coluna normalmente como um dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:26:23.845737Z",
     "start_time": "2021-05-21T00:26:23.840244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finance'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['_1'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "O atributo rdd do DataFrame também nos dá acesso direto à RDD fundamental que constrói o DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:26:43.660095Z",
     "start_time": "2021-05-21T00:26:43.630309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[33] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa RDD podemos aplicar todas as transformações e ações que aprendemos, desde que respeitando o tipo do elemento Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:27:07.222509Z",
     "start_time": "2021-05-21T00:27:07.000299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Finance', _2=10), Row(_1='Marketing', _2=20)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acessando o DataFrame\n",
    "A ideia de termos um DataFrame, contudo, é não precisarmos operar diretamente nas RDDs. Para isso existe um conjunto de funções de acesso direto ao DataFrame. Como estas funções operam intrinsecamente em RDDs, elas também são transformações e ações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show()\n",
    "A ação show() nos permite olhar o DataFrame como uma tabela, muito mais próximo do Pandas DataFrame. Cuidado, contudo, com o desejo de olhar o DataFrame completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:27:44.225248Z",
     "start_time": "2021-05-21T00:27:44.041299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## printSchema()\n",
    "\n",
    "A ação show() não nos mostra claramente qual é o tipo dos dados envolvidos (similar a dtypes). Para isso, podemos imprimir o Schema do DataFrame. \n",
    "\n",
    "O Schema é a definição do tipo de dado de cada coluna. É importante notar que o Spark inferiu esses dados a partir da RDD. O Spark infere a partir do primeiro elemento nos dados. Portanto, é importante que na leitura de arquivos, caso a inferência seja utilizada, garantir que a primeira linha do arquivo possua o tipo correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:06.542170Z",
     "start_time": "2021-05-21T00:28:06.530567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtypes\n",
    "\n",
    "Podemos também utilizar o atributo dtypes para mostrar o tipo de cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:24.737418Z",
     "start_time": "2021-05-21T00:28:24.731344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_1', 'string'), ('_2', 'bigint')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o Schema\n",
    "Idealmente, quando operando em grandes massas de dados, a inferência do tipo pela primeira linha não é adequada e utilizamos como boa prática a definição manual do schema. Para isso precisamos utilizar os tipos de dados nativos do Spark que são convertidos para e de tipos python pela API pyspark.\n",
    "\n",
    "Para isso importamos os tipos do spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:43.544586Z",
     "start_time": "2021-05-21T00:28:43.540393Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StructType(), StructField(), IntegerType(), StringType(), FloatType()\n",
    "Cada tipo de variável é na verdade um objeto de uma classe. Então acessamos estes objetos para instanciar e para identificar os tipos de cada variável. \n",
    "\n",
    "O Schema de um DataFrame é definido como um objeto StructType (similar a uma lista ou tupla) com campos do tipo StructField(). Cada campo (StructField) possui um nome, um tipo (objeto do tipo no spark) e um flag se é permitida a existência de nulos. \n",
    "\n",
    "Assim, podemos definir explicitametne um Schema para nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:14.074827Z",
     "start_time": "2021-05-21T00:31:14.070260Z"
    }
   },
   "outputs": [],
   "source": [
    "my_schema = StructType([ \\\n",
    "                       StructField('dept_name', StringType(), True), \\\n",
    "                       StructField('dept_id', IntegerType(), True), \\\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequencia, atribuímos o nosso schema ao DataFrame na sua criação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:14.959049Z",
     "start_time": "2021-05-21T00:31:14.920363Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf1 = spark.createDataFrame(rdd, schema = my_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então olhar nosso DataFrame e verificar que as colunas agora vêm identificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:16.272213Z",
     "start_time": "2021-05-21T00:31:16.090423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:27.848904Z",
     "start_time": "2021-05-21T00:31:27.843263Z"
    }
   },
   "source": [
    "Podemos também verificar o Schema do DataFrame com as características que definimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:39.338187Z",
     "start_time": "2021-05-21T00:31:39.333233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações básicas com colunas\n",
    "\n",
    "A exemplo do Pandas DataFrame, temos à nossa disposição uma série de transformações e ações que nos permite operar sobre o Spark DataFrame. Veremos a seguir as operações em colunas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns\n",
    "O atributo columns retorna, a exemplo do Pandas, uma lista com o nome das colunas. Contudo, diferente do Pandas, não é possível sobrescrever este atributo diretamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:11.765697Z",
     "start_time": "2021-05-21T00:32:11.760307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumnRenamed()\n",
    "Para renomear uma coluna utilizamos a função withColumnRenamed(). Esta função opera em uma coluna por vez. Assim, podemos utilizar um laço para renomear todas as colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:42.550595Z",
     "start_time": "2021-05-21T00:32:42.531472Z"
    }
   },
   "outputs": [],
   "source": [
    "for old_col, new_col in zip(sdf.columns, ['dept_name', 'dept_id']):\n",
    "    sdf = sdf.withColumnRenamed(old_col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:46.975020Z",
     "start_time": "2021-05-21T00:32:46.790250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop()\n",
    "Podemos descartar colunas usando a função .drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:07.468100Z",
     "start_time": "2021-05-21T00:33:07.230097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|dept_name|\n",
      "+---------+\n",
      "|  Finance|\n",
      "|Marketing|\n",
      "|    Sales|\n",
      "|       IT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.drop('dept_id').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessando colunas\n",
    "Para acessar as colunas de um DataFrame, podemos utilizar a mesma notação python. Observe, contudo, que as colunas são representadas como objetos e, assim, o acesso à coluna não retorna automaticamente os dados existentes nela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:32.526381Z",
     "start_time": "2021-05-21T00:33:32.450232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'dept_id'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf['dept_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:42.625523Z",
     "start_time": "2021-05-21T00:33:42.570266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'dept_id'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.dept_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select()\n",
    "Para acessar os dados de uma coluna específica, precisamos utilizar a transformação select() seguida de uma ação show(). Esta transformação é similiar ao SELECT em SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:58.099007Z",
     "start_time": "2021-05-21T00:33:57.920370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|dept_name|\n",
      "+---------+\n",
      "|  Finance|\n",
      "|Marketing|\n",
      "|    Sales|\n",
      "|       IT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('dept_name').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case sensitivity\n",
    "Observe que a exemplo do SQL, DataFrames em Spark não são naturalmente sensíveis ao caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:34:15.501565Z",
     "start_time": "2021-05-21T00:34:15.320632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|DEPT_NAME|\n",
      "+---------+\n",
      "|  Finance|\n",
      "|Marketing|\n",
      "|    Sales|\n",
      "|       IT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('DEPT_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:34:34.631098Z",
     "start_time": "2021-05-21T00:34:34.440421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|dept_NAME|\n",
      "+---------+\n",
      "|  Finance|\n",
      "|Marketing|\n",
      "|    Sales|\n",
      "|       IT|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('dept_NAME').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações básicas com linhas\n",
    "Da mesma forma que temos operações com colunas, temos operações com linhas.\n",
    "\n",
    "## limit()\n",
    "A transformação limit(LIM) nos permite limitar um número de registros (linhas no DataFrame) a ser retornado. POdemos concatenar com .collect() e temos o DataFrame no formato de RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:35:13.374964Z",
     "start_time": "2021-05-21T00:35:13.200477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dept_name='Finance', dept_id=10),\n",
       " Row(dept_name='Marketing', dept_id=20),\n",
       " Row(dept_name='Sales', dept_id=30)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.limit(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso podemos reduzir o número de linhas do nosso DataFrame para aumentar a velocidade do nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:35:36.000515Z",
     "start_time": "2021-05-21T00:35:35.990571Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = sdf.limit(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter() - filtrando linhas\n",
    "A transformação filter() nos permite filtrar linhas com base em condições especificadas sobre colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:02.834516Z",
     "start_time": "2021-05-21T00:36:02.453420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Marketing|     20|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.filter(sdf.dept_id == 20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## where() - filtrando linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:17.736282Z",
     "start_time": "2021-05-21T00:36:17.730335Z"
    }
   },
   "source": [
    "Para manter a similaridade como o SQL, a transformação filter() também pode ser acessada através de seu apelido (alias) where()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:36.380428Z",
     "start_time": "2021-05-21T00:36:36.170693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.where(sdf.dept_id == 10).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct() - filtrando valores distintos\n",
    "Utilizamos a transformação distinct() para selecionar apenas os valores distintos de uma coluna específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:37:01.885896Z",
     "start_time": "2021-05-21T00:37:01.240361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|dept_id|\n",
      "+-------+\n",
      "|     10|\n",
      "|     20|\n",
      "|     30|\n",
      "|     40|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('dept_id').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## union() - concatenando linhas\n",
    "Para concatenar linhas no DataFrame, precisamos criar uma RDD com objetos do tipo Row e Schema idêntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:38:28.674744Z",
     "start_time": "2021-05-21T00:38:28.670193Z"
    }
   },
   "outputs": [],
   "source": [
    "new_rows = [Row(dept_name = 'Production', dept_id = 50), \n",
    "            Row(dept_name = 'Human Resources', dept_id = 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:38:34.775686Z",
     "start_time": "2021-05-21T00:38:34.770448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dept_name='Production', dept_id=50),\n",
       " Row(dept_name='Human Resources', dept_id=60)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos essa lista em uma RDD, usando parallelize(), da mesma forma que fizemos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:00.869358Z",
     "start_time": "2021-05-21T00:39:00.860407Z"
    }
   },
   "outputs": [],
   "source": [
    "parallelizedRows = spark.sparkContext.parallelize(new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, criamos o DataFrame, definindo o Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:28.912117Z",
     "start_time": "2021-05-21T00:39:28.870363Z"
    }
   },
   "outputs": [],
   "source": [
    "newDF = spark.createDataFrame(parallelizedRows, my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:35.790342Z",
     "start_time": "2021-05-21T00:39:35.630421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      dept_name|dept_id|\n",
      "+---------------+-------+\n",
      "|     Production|     50|\n",
      "|Human Resources|     60|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então concatenar nosso novo DataFrame ao DataFrame antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:02.349584Z",
     "start_time": "2021-05-21T00:40:02.283006Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf1 = sdf.union(newDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:11.145556Z",
     "start_time": "2021-05-21T00:40:10.701273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      dept_name|dept_id|\n",
      "+---------------+-------+\n",
      "|        Finance|     10|\n",
      "|      Marketing|     20|\n",
      "|          Sales|     30|\n",
      "|             IT|     40|\n",
      "|     Production|     50|\n",
      "|Human Resources|     60|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orderBy() - ordenando linhas\n",
    "Com a transformação orderBy() podemos ordenar as linhas do DataFrame. Para isso, precisamos utilizar os métodos .asc() ou .desc() na coluna chave, indicando a ordem desejada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:48.398791Z",
     "start_time": "2021-05-21T00:40:48.000270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      dept_name|dept_id|\n",
      "+---------------+-------+\n",
      "|Human Resources|     60|\n",
      "|     Production|     50|\n",
      "|             IT|     40|\n",
      "|          Sales|     30|\n",
      "|      Marketing|     20|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.orderBy(sdf1.dept_id.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark.sql.functions\n",
    "Existe uma série de funções que facilitam a manipulação de DataFrames. Elas estão disponíveis no pacote pyspark.sql.functions e podem ser investigadas na referência abaixo:\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "Podemos importá-las todas através da linha de comando abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:41:27.216458Z",
     "start_time": "2021-05-21T00:41:27.200290Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lit() - criando colunas\n",
    "Como visto anteriormente, o Spark possui tipos próprios de dados com correspondência com o Python. Se quisermos criar uma coluna constante, podemos evitar o trabalho de criar a coluna em Python e transformá-la, utilizando a função lit(val) que cria uma coluna de literais (constantes) val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:41:44.868609Z",
     "start_time": "2021-05-21T00:41:44.820337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'10'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.lit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn() - adicionando colunas\n",
    "\n",
    "Para isso, precisamos concatenar uma coluna de literais. Isso é feito com a transformação withColumn(nome, nova_col)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:42:40.713566Z",
     "start_time": "2021-05-21T00:42:40.373309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----+\n",
      "|dept_name|dept_id|ones|\n",
      "+---------+-------+----+\n",
      "|  Finance|     10|   1|\n",
      "|Marketing|     20|   1|\n",
      "|    Sales|     30|   1|\n",
      "+---------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.withColumn('ones', sf.lit(1)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## col() - acessando colunas\n",
    "\n",
    "Em alguns casos precisamos explicitar que desejamos acessar um objeto do tipo Column. Para isso temos à nossa disposição a função .col(nome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:42:44.644237Z",
     "start_time": "2021-05-21T00:42:44.250259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|dept_id|\n",
      "+-------+\n",
      "|     10|\n",
      "|     20|\n",
      "|     30|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.select(sf.col('dept_id')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que temos, com isso, diferentes formas de acessar colunas no select. Em alguns casos específicos será necessário optar pela menção explícita ao tipo do objeto usando .col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:43:16.281709Z",
     "start_time": "2021-05-21T00:43:15.967344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+\n",
      "|dept_id|dept_name|dept_id|\n",
      "+-------+---------+-------+\n",
      "|     10|  Finance|     10|\n",
      "|     20|Marketing|     20|\n",
      "|     30|    Sales|     30|\n",
      "+-------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.select(sdf1.dept_id, sf.col('dept_name'), 'dept_id').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ainda acessar a RDD resultante da transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:43:53.141443Z",
     "start_time": "2021-05-21T00:43:52.861402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dept_id=10, dept_id=10, dept_id=10),\n",
       " Row(dept_id=20, dept_id=20, dept_id=20),\n",
       " Row(dept_id=30, dept_id=30, dept_id=30)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.select(sdf1.dept_id, sf.col('dept_id'), 'dept_id').rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alias()\n",
    "\n",
    "A exemplo do SQL, podemos utilizar o método .alias() de uma coluna específica para renomear colunas rapidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:45:43.459353Z",
     "start_time": "2021-05-21T00:45:43.260528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+\n",
      "|dept_name|dept_id|identificador|\n",
      "+---------+-------+-------------+\n",
      "|  Finance|     10|           10|\n",
      "|Marketing|     20|           20|\n",
      "|    Sales|     30|           30|\n",
      "|       IT|     40|           40|\n",
      "+---------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('*', sf.col('dept_id').alias('identificador')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressões\n",
    "DataFrames são equivalentes de tabelas no Pandas e também de tabelas no SQL. Buscando manter compatibilidade com o SQL, expressões permitem que escrevamos algumas expressões simplificadas de SQL para operar no DataFrame.\n",
    "\n",
    "\n",
    "## expr()\n",
    "Para isso, utilizamos a função expr(). Podemos, por exemplo, renomear colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:54:44.670058Z",
     "start_time": "2021-05-21T00:54:43.897373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|departamento|\n",
      "+------------+\n",
      "|     Finance|\n",
      "|   Marketing|\n",
      "|       Sales|\n",
      "|          IT|\n",
      "|  Production|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.select(sf.expr('dept_name as departamento')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também operar sobre colunas diretamente, a exemplo do SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:55:25.186146Z",
     "start_time": "2021-05-21T00:55:24.900466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+\n",
      "|dept_id|dept_name|(dept_id / 10)|\n",
      "+-------+---------+--------------+\n",
      "|     10|  Finance|           1.0|\n",
      "|     20|Marketing|           2.0|\n",
      "|     30|    Sales|           3.0|\n",
      "+-------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1.select('dept_id', 'dept_name', sf.expr('dept_id / 10')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selectExpr()\n",
    "\n",
    "Como a combinação de .select() e .expr() é muito comum, o spark já nos fornece um atalho: selectExpr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:56:23.388730Z",
     "start_time": "2021-05-21T00:56:23.180428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+\n",
      "|dept_name|dept_id|id by 10|\n",
      "+---------+-------+--------+\n",
      "|  Finance|     10|     1.0|\n",
      "|Marketing|     20|     2.0|\n",
      "|    Sales|     30|     3.0|\n",
      "|       IT|     40|     4.0|\n",
      "+---------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.selectExpr('*', 'dept_id / 10 as `id by 10`').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrindo arquivos CSV\n",
    "Até agora trabalhamos com DataFrames criados manualmente a partir de uma RDD prévia. Podemos utilizar as funções do pacote .read do Spark para ler diretamente fontes de dados. Para carregar arquivos .csv usamos a função .csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:56:53.756026Z",
     "start_time": "2021-05-21T00:56:53.310648Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('../10_dados/movie_lens/ratings.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:53:47.626787Z",
     "start_time": "2021-05-20T19:53:47.565414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, movieId: string, rating: string, timestamp: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:53:59.440442Z",
     "start_time": "2021-05-20T19:53:59.015732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:54:11.793463Z",
     "start_time": "2021-05-20T19:54:11.535305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='1', movieId='296', rating='5.0', timestamp='1147880044'),\n",
       " Row(userId='1', movieId='306', rating='3.5', timestamp='1147868817'),\n",
       " Row(userId='1', movieId='307', rating='5.0', timestamp='1147868828'),\n",
       " Row(userId='1', movieId='665', rating='5.0', timestamp='1147878820'),\n",
       " Row(userId='1', movieId='899', rating='3.5', timestamp='1147868510')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:02.741016Z",
     "start_time": "2021-05-20T19:55:02.735348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId', 'movieId', 'rating', 'timestamp']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:09.291075Z",
     "start_time": "2021-05-20T19:55:09.285145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'string'),\n",
       " ('movieId', 'string'),\n",
       " ('rating', 'string'),\n",
       " ('timestamp', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:18.054126Z",
     "start_time": "2021-05-20T19:55:18.045350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:58:59.859416Z",
     "start_time": "2021-05-20T19:58:59.855595Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, TimestampType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:29.759760Z",
     "start_time": "2021-05-20T20:25:29.755337Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = (('userId', IntegerType()),\n",
    "          ('movieId', IntegerType()),\n",
    "          ('rating', DoubleType()),\n",
    "          ('timestamp', StringType())\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:30.061298Z",
     "start_time": "2021-05-20T20:25:30.055250Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(x[0], x[1], True) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:30.480166Z",
     "start_time": "2021-05-20T20:25:30.475509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(userId,IntegerType,true),StructField(movieId,IntegerType,true),StructField(rating,DoubleType,true),StructField(timestamp,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:31.466072Z",
     "start_time": "2021-05-20T20:25:31.435433Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('../10_dados/movie_lens/ratings.csv', header=True, schema = schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:31.921871Z",
     "start_time": "2021-05-20T20:25:31.916617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:34.493677Z",
     "start_time": "2021-05-20T20:25:34.125235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:27:11.017949Z",
     "start_time": "2021-05-20T20:27:10.725449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('userId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:27:57.440840Z",
     "start_time": "2021-05-20T20:27:57.028609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|rating|\n",
      "+-------+------+\n",
      "|    296|   5.0|\n",
      "|    306|   3.5|\n",
      "|    307|   5.0|\n",
      "|    665|   5.0|\n",
      "|    899|   3.5|\n",
      "+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['movieId', 'rating']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:20.465692Z",
     "start_time": "2021-05-20T20:29:20.445417Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:48.461981Z",
     "start_time": "2021-05-20T20:29:48.443655Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('new_column', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:49.315111Z",
     "start_time": "2021-05-20T20:29:49.035698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|new_column|\n",
      "+------+-------+------+----------+----------+\n",
      "|     1|    296|   5.0|1147880044|         1|\n",
      "|     1|    306|   3.5|1147868817|         1|\n",
      "|     1|    307|   5.0|1147868828|         1|\n",
      "|     1|    665|   5.0|1147878820|         1|\n",
      "|     1|    899|   3.5|1147868510|         1|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:31:25.436192Z",
     "start_time": "2021-05-20T20:31:25.385370Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "df = df.withColumn('concat', concat(col('movieId'), lit('-'), col('rating')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:31:30.884229Z",
     "start_time": "2021-05-20T20:31:30.475631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+-------+\n",
      "|userId|movieId|rating| timestamp|new_column| concat|\n",
      "+------+-------+------+----------+----------+-------+\n",
      "|     1|    296|   5.0|1147880044|         1|296-5.0|\n",
      "|     1|    306|   3.5|1147868817|         1|306-3.5|\n",
      "|     1|    307|   5.0|1147868828|         1|307-5.0|\n",
      "|     1|    665|   5.0|1147878820|         1|665-5.0|\n",
      "|     1|    899|   3.5|1147868510|         1|899-3.5|\n",
      "+------+-------+------+----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:32:08.470291Z",
     "start_time": "2021-05-20T20:32:08.455277Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('new_column', 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:32:11.036945Z",
     "start_time": "2021-05-20T20:32:10.878419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------+-------+\n",
      "|userId|movieId|rating| timestamp|constant| concat|\n",
      "+------+-------+------+----------+--------+-------+\n",
      "|     1|    296|   5.0|1147880044|       1|296-5.0|\n",
      "|     1|    306|   3.5|1147868817|       1|306-3.5|\n",
      "|     1|    307|   5.0|1147868828|       1|307-5.0|\n",
      "|     1|    665|   5.0|1147878820|       1|665-5.0|\n",
      "|     1|    899|   3.5|1147868510|       1|899-3.5|\n",
      "+------+-------+------+----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:33:18.929177Z",
     "start_time": "2021-05-20T20:32:49.955959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieId|count|\n",
      "+-------+-----+\n",
      "|   1088|11935|\n",
      "|   1580|40308|\n",
      "|   3175|14659|\n",
      "|  44022| 4833|\n",
      "| 175197|  610|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('movieId').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:33:56.640954Z",
     "start_time": "2021-05-20T20:33:49.385757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000095"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:35:09.794484Z",
     "start_time": "2021-05-20T20:35:09.505251Z"
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('movielens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:36:18.533696Z",
     "start_time": "2021-05-20T20:36:18.515114Z"
    }
   },
   "outputs": [],
   "source": [
    "query = spark.sql('select * from movielens limit 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:37:56.319942Z",
     "start_time": "2021-05-20T20:37:56.295157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit (3)\n",
      "+- * Project (2)\n",
      "   +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [userId#191, movieId#192, rating#193, timestamp#194]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv]\n",
      "ReadSchema: struct<userId:int,movieId:int,rating:double,timestamp:string>\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [6]: [userId#191, movieId#192, rating#193, timestamp#194, 1 AS constant#334, concat(cast(movieId#192 as string), -, cast(rating#193 as string)) AS concat#298]\n",
      "Input [4]: [userId#191, movieId#192, rating#193, timestamp#194]\n",
      "\n",
      "(3) CollectLimit\n",
      "Input [6]: [userId#191, movieId#192, rating#193, timestamp#194, constant#334, concat#298]\n",
      "Arguments: 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:38:39.410096Z",
     "start_time": "2021-05-20T20:38:39.098228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------+-------+\n",
      "|userId|movieId|rating| timestamp|constant| concat|\n",
      "+------+-------+------+----------+--------+-------+\n",
      "|     1|    296|   5.0|1147880044|       1|296-5.0|\n",
      "|     1|    306|   3.5|1147868817|       1|306-3.5|\n",
      "|     1|    307|   5.0|1147868828|       1|307-5.0|\n",
      "|     1|    665|   5.0|1147878820|       1|665-5.0|\n",
      "|     1|    899|   3.5|1147868510|       1|899-3.5|\n",
      "+------+-------+------+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:00.169356Z",
     "start_time": "2021-05-20T20:42:00.055291Z"
    }
   },
   "outputs": [],
   "source": [
    "query = spark.sql(\"\"\"select movieId, count(*)\n",
    "                     from movielens \n",
    "                     where rating = 5\n",
    "                     group by movieId \n",
    "                     order by 2 desc \n",
    "                     limit 5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:01.808240Z",
     "start_time": "2021-05-20T20:42:01.700936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject (7)\n",
      "+- * HashAggregate (6)\n",
      "   +- Exchange (5)\n",
      "      +- * HashAggregate (4)\n",
      "         +- * Project (3)\n",
      "            +- * Filter (2)\n",
      "               +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [movieId#192, rating#193]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/data/aluno/shared/10_dados/movie_lens/ratings.csv]\n",
      "PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)]\n",
      "ReadSchema: struct<movieId:int,rating:double>\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [2]: [movieId#192, rating#193]\n",
      "Condition : (isnotnull(rating#193) AND (rating#193 = 5.0))\n",
      "\n",
      "(3) Project [codegen id : 1]\n",
      "Output [1]: [movieId#192]\n",
      "Input [2]: [movieId#192, rating#193]\n",
      "\n",
      "(4) HashAggregate [codegen id : 1]\n",
      "Input [1]: [movieId#192]\n",
      "Keys [1]: [movieId#192]\n",
      "Functions [1]: [partial_count(1)]\n",
      "Aggregate Attributes [1]: [count#463L]\n",
      "Results [2]: [movieId#192, count#464L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [2]: [movieId#192, count#464L]\n",
      "Arguments: hashpartitioning(movieId#192, 200), true, [id=#334]\n",
      "\n",
      "(6) HashAggregate [codegen id : 2]\n",
      "Input [2]: [movieId#192, count#464L]\n",
      "Keys [1]: [movieId#192]\n",
      "Functions [1]: [count(1)]\n",
      "Aggregate Attributes [1]: [count(1)#458L]\n",
      "Results [2]: [movieId#192, count(1)#458L AS count(1)#459L]\n",
      "\n",
      "(7) TakeOrderedAndProject\n",
      "Input [2]: [movieId#192, count(1)#459L]\n",
      "Arguments: 5, [count(1)#459L DESC NULLS LAST], [movieId#192, count(1)#459L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:28.042780Z",
     "start_time": "2021-05-20T20:42:04.635241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|movieId|count(1)|\n",
      "+-------+--------+\n",
      "|    318|   39553|\n",
      "|    296|   32169|\n",
      "|    356|   25918|\n",
      "|    260|   25804|\n",
      "|   2571|   25482|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:43:42.209948Z",
     "start_time": "2021-05-20T20:43:42.205681Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:45:54.751926Z",
     "start_time": "2021-05-20T20:45:29.688390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>movieId</th><th>count(1)</th></tr>\n",
       "<tr><td>318</td><td>39553</td></tr>\n",
       "<tr><td>296</td><td>32169</td></tr>\n",
       "<tr><td>356</td><td>25918</td></tr>\n",
       "<tr><td>260</td><td>25804</td></tr>\n",
       "<tr><td>2571</td><td>25482</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[movieId: int, count(1): bigint]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:46:00.514313Z",
     "start_time": "2021-05-20T20:46:00.395710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>movieId</th><th>count(1)</th></tr>\n",
       "<tr><td>318</td><td>39553</td></tr>\n",
       "<tr><td>296</td><td>32169</td></tr>\n",
       "<tr><td>356</td><td>25918</td></tr>\n",
       "<tr><td>260</td><td>25804</td></tr>\n",
       "<tr><td>2571</td><td>25482</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[movieId: int, count(1): bigint]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:46:51.203492Z",
     "start_time": "2021-05-20T20:46:51.179137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[count(1)#459L DESC NULLS LAST], output=[movieId#192,count(1)#459L])\n",
      "+- *(2) HashAggregate(keys=[movieId#192], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(movieId#192, 200), true, [id=#334]\n",
      "      +- *(1) HashAggregate(keys=[movieId#192], functions=[partial_count(1)])\n",
      "         +- *(1) Project [movieId#192]\n",
      "            +- *(1) Filter (isnotnull(rating#193) AND (rating#193 = 5.0))\n",
      "               +- FileScan csv [movieId#192,rating#193] Batched: false, DataFilters: [isnotnull(rating#193), (rating#193 = 5.0)], Format: CSV, Location: InMemoryFileIndex[file:/data/aluno/shared/10_dados/movie_lens/ratings.csv], PartitionFilters: [], PushedFilters: [IsNotNull(rating), EqualTo(rating,5.0)], ReadSchema: struct<movieId:int,rating:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:49:52.961714Z",
     "start_time": "2021-05-20T20:49:28.855213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>movieId</th><th>count(1)</th></tr>\n",
       "<tr><td>318</td><td>39553</td></tr>\n",
       "<tr><td>296</td><td>32169</td></tr>\n",
       "<tr><td>356</td><td>25918</td></tr>\n",
       "<tr><td>260</td><td>25804</td></tr>\n",
       "<tr><td>2571</td><td>25482</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[movieId: int, count(1): bigint]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:48:45.231492Z",
     "start_time": "2021-05-20T20:48:21.805178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>movieId</th><th>count(1)</th></tr>\n",
       "<tr><td>318</td><td>39553</td></tr>\n",
       "<tr><td>296</td><td>32169</td></tr>\n",
       "<tr><td>356</td><td>25918</td></tr>\n",
       "<tr><td>260</td><td>25804</td></tr>\n",
       "<tr><td>2571</td><td>25482</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[movieId: int, count(1): bigint]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Insper (PySpark)",
   "language": "python",
   "name": "insper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
